{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:54:49.118737600Z",
     "start_time": "2023-11-20T22:54:47.588276Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Flatten, Dropout, Conv2D  ,MaxPooling2D, Reshape\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3668\\2579927642.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'activity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_act\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sub_activity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_act\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# print(df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\goyal\\anaconda3\\envs\\pdiot\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "respeck_filepaths = glob.glob(\"../Respeck/*\")\n",
    "df1 = pd.DataFrame()\n",
    "for rfp in respeck_filepaths:\n",
    "    files = glob.glob(f\"{rfp}/*\")\n",
    "    \n",
    "    for file in files:\n",
    "        [main_act,sub_act] = file.split(\".csv\")[0].split('_')[-2:]\n",
    "        # main_activity = file.split(\".csv\")[0].split('_')[-2]\n",
    "        \n",
    "        df = pd.read_csv(file,index_col=0)\n",
    "        df['activity'] = main_act\n",
    "        df['sub_activity'] = sub_act\n",
    "        df['user'] = rfp.split('\\\\')[-1]\n",
    "        # print(df)\n",
    "        df1 = df1.concat(df)\n",
    "\n",
    "df1 = df1[df1['sub_activity'] == 'breathingNormal']     \n",
    "df1.loc[df1['activity'].isin(('sitting', 'standing')),'activity'] = 'sitting_standing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['user','activity','timestamp', 'accel_x', 'accel_y', 'accel_z']\n",
    "# df1 = df1[columns]\n",
    "df_har = df1[columns]\n",
    "# removing null values\n",
    "df_har = df_har.dropna()\n",
    "df_har.shape\n",
    "# transforming the user to float\n",
    "df_har['user'] = df_har['user'].str.replace('s', '')\n",
    "df_har['user'] = df_har['user'].apply(lambda x:int(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_har.to_csv('task1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>accel_x</th>\n",
       "      <th>accel_y</th>\n",
       "      <th>accel_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ascending</td>\n",
       "      <td>1697605965</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>-0.855774</td>\n",
       "      <td>-0.029846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ascending</td>\n",
       "      <td>1697606005</td>\n",
       "      <td>-0.001709</td>\n",
       "      <td>-0.826233</td>\n",
       "      <td>-0.036194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ascending</td>\n",
       "      <td>1697606045</td>\n",
       "      <td>-0.058838</td>\n",
       "      <td>-0.933899</td>\n",
       "      <td>-0.032532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ascending</td>\n",
       "      <td>1697606085</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>-1.115051</td>\n",
       "      <td>-0.028870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ascending</td>\n",
       "      <td>1697606125</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>-1.035217</td>\n",
       "      <td>-0.076477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639844</th>\n",
       "      <td>98</td>\n",
       "      <td>sitting_standing</td>\n",
       "      <td>1697636165</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>-0.985901</td>\n",
       "      <td>0.050232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639845</th>\n",
       "      <td>98</td>\n",
       "      <td>sitting_standing</td>\n",
       "      <td>1697636205</td>\n",
       "      <td>-0.061523</td>\n",
       "      <td>-0.980042</td>\n",
       "      <td>0.061951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639846</th>\n",
       "      <td>98</td>\n",
       "      <td>sitting_standing</td>\n",
       "      <td>1697636245</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>-1.000793</td>\n",
       "      <td>0.043396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639847</th>\n",
       "      <td>98</td>\n",
       "      <td>sitting_standing</td>\n",
       "      <td>1697636285</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>-0.976379</td>\n",
       "      <td>0.068054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639848</th>\n",
       "      <td>98</td>\n",
       "      <td>sitting_standing</td>\n",
       "      <td>1697636325</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>-0.978821</td>\n",
       "      <td>0.046814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>639849 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user          activity   timestamp   accel_x   accel_y   accel_z\n",
       "0          1         ascending  1697605965  0.011963 -0.855774 -0.029846\n",
       "1          1         ascending  1697606005 -0.001709 -0.826233 -0.036194\n",
       "2          1         ascending  1697606045 -0.058838 -0.933899 -0.032532\n",
       "3          1         ascending  1697606085 -0.002441 -1.115051 -0.028870\n",
       "4          1         ascending  1697606125 -0.036621 -1.035217 -0.076477\n",
       "...      ...               ...         ...       ...       ...       ...\n",
       "639844    98  sitting_standing  1697636165 -0.057617 -0.985901  0.050232\n",
       "639845    98  sitting_standing  1697636205 -0.061523 -0.980042  0.061951\n",
       "639846    98  sitting_standing  1697636245 -0.067627 -1.000793  0.043396\n",
       "639847    98  sitting_standing  1697636285 -0.057617 -0.976379  0.068054\n",
       "639848    98  sitting_standing  1697636325 -0.056396 -0.978821  0.046814\n",
       "\n",
       "[639849 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY RUN THIS AFTER CSV GENERATION\n",
    "general_act_df = pd.read_csv('task1.csv')\n",
    "general_act_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42   \n",
    "n_time_steps = 50 \n",
    "n_features = 3 \n",
    "step = 10\n",
    "n_epochs = 20      \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLabelEncoder:\n",
    "    def _init_(self):\n",
    "        self.class_mapping = {\n",
    "            'sitting_standing': 0,\n",
    "            'lyingLeft': 1,\n",
    "            'lyingRight': 2,\n",
    "            'lyingBack': 3,\n",
    "            'lyingStomach': 4,\n",
    "            'normalWalking': 5,\n",
    "            'running': 6,\n",
    "            'descending': 7,\n",
    "            'ascending': 8,\n",
    "            'shuffleWalking': 9,\n",
    "            'miscMovement':10\n",
    "        }\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return [self.class_mapping[cls] for cls in y]\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        reverse_mapping = {v: k for k, v in self.class_mapping.items()}\n",
    "        return [reverse_mapping[val] for val in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments_overlap(data):\n",
    "    segments = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0,  data.shape[0]- n_time_steps, step):  \n",
    "\n",
    "        xs = data['accel_x'].values[i: i + n_time_steps]\n",
    "\n",
    "        ys = data['accel_y'].values[i: i + n_time_steps]\n",
    "\n",
    "        zs = data['accel_z'].values[i: i + n_time_steps]\n",
    "\n",
    "        label = stats.mode(data['activity'][i: i + n_time_steps])[0][0]\n",
    "\n",
    "        segments.append([xs, ys, zs])\n",
    "\n",
    "        labels.append(label)\n",
    "        \n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, n_time_steps, n_features)\n",
    "    labels = np.asarray(labels).reshape(-1,1)\n",
    "\n",
    "    # le = LabelEncoder()\n",
    "    # labels = le.fit_transform(labels)\n",
    "    enc = OneHotEncoder(handle_unknown='ignore').fit(labels)\n",
    "    labels = enc.transform(labels).toarray()\n",
    "    # labels = np.asarray(pd.get_dummies(labels), dtype = np.float32)\n",
    "    # print(enc.categories_)\n",
    "    return reshaped_segments,labels,enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments_no_overlap(data):\n",
    "    segments= []\n",
    "    labels = []\n",
    "    \n",
    "    activities = data['activity'].unique()\n",
    "    for act in activities:\n",
    "        act_data = data[data['activity'] == act]\n",
    "        \n",
    "        # for i in range(0, len(all_data) - n_time_steps, step):\n",
    "        for i in range(0,  act_data.shape[0]- n_time_steps, step):  \n",
    "\n",
    "            xs = act_data['accel_x'].values[i: i + n_time_steps]\n",
    "            ys = act_data['accel_y'].values[i: i + n_time_steps]\n",
    "            zs = act_data['accel_z'].values[i: i + n_time_steps]\n",
    "\n",
    "            segments.append([xs, ys, zs])\n",
    "            labels.append(act)\n",
    "\n",
    "    #reshape the segments which is (list of arrays) to a list\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, n_time_steps, n_features)\n",
    "\n",
    "    labels = np.asarray(pd.get_dummies(labels), dtype = np.float32)\n",
    "    \n",
    "    return reshaped_segments,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def test_train_split(seg,labls):    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(seg, labls, test_size = 0.2, random_state = random_seed)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM(X_train,y_train):\n",
    "    model = Sequential()\n",
    "    # RNN layer\n",
    "    model.add(LSTM(units = 128, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    # Dropout layer\n",
    "    model.add(Dropout(0.5)) \n",
    "    # Dense layer with ReLu\n",
    "    model.add(Dense(units = 64, activation='relu'))\n",
    "    # Softmax layer\n",
    "    model.add(Dense(y_train.shape[1], activation = 'softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn(trainX, trainy):\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((n_timesteps, n_features, 1)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,1), activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,1), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=n_epochs, batch_size=batch_size, verbose=1)\n",
    "    # evaluate model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1971/1971 [==============================] - 27s 13ms/step - loss: 0.4678 - accuracy: 0.8346\n",
      "Epoch 2/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.2695 - accuracy: 0.9061\n",
      "Epoch 3/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.2080 - accuracy: 0.9276\n",
      "Epoch 4/20\n",
      "1971/1971 [==============================] - 29s 15ms/step - loss: 0.1727 - accuracy: 0.9409\n",
      "Epoch 5/20\n",
      "1971/1971 [==============================] - 29s 15ms/step - loss: 0.1475 - accuracy: 0.9491\n",
      "Epoch 6/20\n",
      "1971/1971 [==============================] - 30s 15ms/step - loss: 0.1267 - accuracy: 0.9563\n",
      "Epoch 7/20\n",
      "1971/1971 [==============================] - 29s 15ms/step - loss: 0.1153 - accuracy: 0.9609\n",
      "Epoch 8/20\n",
      "1971/1971 [==============================] - 30s 15ms/step - loss: 0.1031 - accuracy: 0.9646\n",
      "Epoch 9/20\n",
      "1971/1971 [==============================] - 30s 15ms/step - loss: 0.0931 - accuracy: 0.9667\n",
      "Epoch 10/20\n",
      "1971/1971 [==============================] - 29s 15ms/step - loss: 0.0859 - accuracy: 0.9698\n",
      "Epoch 11/20\n",
      "1971/1971 [==============================] - 27s 14ms/step - loss: 0.0779 - accuracy: 0.9730\n",
      "Epoch 12/20\n",
      "1971/1971 [==============================] - 25s 13ms/step - loss: 0.0722 - accuracy: 0.9743\n",
      "Epoch 13/20\n",
      "1971/1971 [==============================] - 25s 13ms/step - loss: 0.0694 - accuracy: 0.9756\n",
      "Epoch 14/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.0640 - accuracy: 0.9776\n",
      "Epoch 15/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.0631 - accuracy: 0.9782\n",
      "Epoch 16/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.0568 - accuracy: 0.9802\n",
      "Epoch 17/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.0554 - accuracy: 0.9800\n",
      "Epoch 18/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.0527 - accuracy: 0.9799\n",
      "Epoch 19/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.0511 - accuracy: 0.9815\n",
      "Epoch 20/20\n",
      "1971/1971 [==============================] - 26s 13ms/step - loss: 0.0509 - accuracy: 0.9818\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9614\n",
      "Test Accuracy (1): 0.9614112377166748\n",
      "Test Loss (1): 0.10917282849550247\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for user in general_act_df['user'].unique():\n",
    "    \n",
    "    train_df = general_act_df[general_act_df['user'] != user]\n",
    "    test_df = general_act_df[general_act_df['user'] == user]\n",
    "\n",
    "    X_train, y_train, categories = segments_overlap(train_df)\n",
    "    X_test, y_test, cat2 = segments_overlap(test_df)\n",
    "    \n",
    "    # model = model_LSTM(X_train,y_train)\n",
    "    # history = model.fit(X_train, y_train, epochs = n_epochs, validation_split = 0.20, batch_size = batch_size, verbose = 1)\n",
    "    \n",
    "    model  = model_cnn(X_train,y_train)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, batch_size = batch_size, verbose = 1)\n",
    "    print(f\"Test Accuracy ({user}):\", accuracy)\n",
    "    print(f\"Test Loss ({user}):\", loss)\n",
    "    \n",
    "    # accuracies.append((user,loss,accuracy))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['ascending', 'descending', 'lyingBack', 'lyingLeft', 'lyingRight',\n",
       "        'lyingStomach', 'miscMovement', 'normalWalking', 'running',\n",
       "        'shuffleWalking', 'sitting_standing'], dtype='<U16')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_4 (QuantizeL  (None, 50, 3)            3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_reshape_7 (QuantizeWr  (None, 50, 3, 1)         1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_14 (QuantizeWr  (None, 48, 3, 64)        387       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_15 (QuantizeWr  (None, 46, 3, 64)        12483     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dropout_11 (QuantizeW  (None, 46, 3, 64)        1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_max_pooling2d_6 (Quan  (None, 23, 3, 64)        1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_flatten_11 (QuantizeW  (None, 4416)             1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_22 (QuantizeWra  (None, 100)              441705    \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense_23 (QuantizeWra  (None, 11)               1116      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 455,698\n",
      "Trainable params: 455,419\n",
      "Non-trainable params: 279\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='sgd',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, reshape_7_layer_call_fn, reshape_7_layer_call_and_return_conditional_losses, conv2d_14_layer_call_fn, conv2d_14_layer_call_and_return_conditional_losses while saving (showing 5 of 17). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\chhal\\AppData\\Local\\Temp\\tmpruc2c2yz\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\chhal\\AppData\\Local\\Temp\\tmpruc2c2yz\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\chhal\\AppData\\Local\\Temp\\tmp7vr0w4hs\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\chhal\\AppData\\Local\\Temp\\tmp7vr0w4hs\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(f'../models/cnn_model_task{1}_{n_time_steps}_{n_features}.tflite', 'wb') as f:\n",
    "  f.write(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00304507 0.00304507 0.00710347 0.00780451 0.00304507 0.05904093\n",
      "  0.00304507 0.00304507 0.00304507 0.00304507 0.90473557]]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"../models/cnn_model_quant1_task1_50_3.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on one instance\n",
    "interpreter.set_tensor(input_details[0]['index'], np.float32(X_test[0:1]))\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.16427783902976847\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    interpreter.set_tensor(input_details[0]['index'], np.float32(X_test[i:i+1]))\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    total_predictions += 1\n",
    "    if np.argmax(output_data) == np.argmax(y_test[i]):\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdiot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
